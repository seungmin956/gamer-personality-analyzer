import json
import pandas as pd
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import numpy as np
import time

def load_interview_data(filename="interview_dataset.json"):
    """ë©´ì ‘ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    
    with open(filename, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # í…ìŠ¤íŠ¸ì™€ ë¼ë²¨ ì¶”ì¶œ
    texts = []
    labels = []
    
    for session in data:
        personality = session['personality_type']
        
        # ëª¨ë“  ë‹µë³€ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        full_answer = " ".join([qa['answer'] for qa in session['questions_answers']])
        
        texts.append(full_answer)
        labels.append(personality)
    
    return texts, labels

def test_baseline_model(X_train, X_test, y_train, y_test):
    """ê¸°ì¤€ ëª¨ë¸ (TF-IDF + Logistic Regression) í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ”§ ê¸°ì¤€ ëª¨ë¸ (TF-IDF + Logistic Regression) í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    # TF-IDF ë²¡í„°í™”
    vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)
    
    # ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµ
    start_time = time.time()
    lr_model = LogisticRegression(random_state=42, max_iter=1000)
    lr_model.fit(X_train_tfidf, y_train)
    
    # ì˜ˆì¸¡
    y_pred = lr_model.predict(X_test_tfidf)
    training_time = time.time() - start_time
    
    # ì„±ëŠ¥ í‰ê°€
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"âœ… ê¸°ì¤€ ëª¨ë¸ ê²°ê³¼:")
    print(f"   ì •í™•ë„: {accuracy:.3f}")
    print(f"   í•™ìŠµ ì‹œê°„: {training_time:.2f}ì´ˆ")
    print(f"   ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y_test, y_pred, zero_division=0))
    
    return accuracy

def test_huggingface_model(texts, labels, model_name):
    """Hugging Face ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    print(f"\nğŸ¤– {model_name} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        # ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ (í•œêµ­ì–´ ëª¨ë¸ì˜ ê²½ìš°)
        if "klue" in model_name or "beomi" in model_name:
            # í•œêµ­ì–´ ëª¨ë¸ìš© - ì§ì ‘ ë¶„ë¥˜ê¸° êµ¬ì„±
            classifier = pipeline(
                "text-classification", 
                model=model_name,
                tokenizer=model_name,
                return_all_scores=True
            )
        else:
            # ì˜ì–´ ëª¨ë¸ìš©
            classifier = pipeline(
                "zero-shot-classification",
                model=model_name
            )
        
        # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (ì „ì²´ ë°ì´í„°ëŠ” ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼)
        sample_size = min(10, len(texts))
        sample_texts = texts[:sample_size]
        sample_labels = labels[:sample_size]
        
        predictions = []
        start_time = time.time()
        
        personality_types = ["ì ê·¹í˜•", "ì‹ ì¤‘í˜•", "ë…¼ë¦¬í˜•", "ê°ì„±í˜•", "ë¦¬ë”í˜•", "íŒ”ë¡œì›Œí˜•"]
        
        for text in sample_texts:
            if "zero-shot" in str(type(classifier)):
                # Zero-shot ë¶„ë¥˜
                result = classifier(text, personality_types)
                predicted_label = result['labels'][0]
            else:
                # ì¼ë°˜ ë¶„ë¥˜ (ì„ì‹œë¡œ ê°ì • ê¸°ë°˜ ë§¤í•‘)
                result = classifier(text[:512])  # í† í° ê¸¸ì´ ì œí•œ
                
                # ê°ì • ì ìˆ˜ë¥¼ ì„±í–¥ìœ¼ë¡œ ë§¤í•‘ (ì„ì‹œ)
                if isinstance(result, list) and len(result) > 0:
                    score = result[0]['score'] if result[0]['label'] == 'POSITIVE' else 1 - result[0]['score']
                    if score > 0.7:
                        predicted_label = "ì ê·¹í˜•"
                    elif score > 0.5:
                        predicted_label = "ë…¼ë¦¬í˜•"
                    else:
                        predicted_label = "ì‹ ì¤‘í˜•"
                else:
                    predicted_label = "ë…¼ë¦¬í˜•"  # ê¸°ë³¸ê°’
            
            predictions.append(predicted_label)
        
        inference_time = time.time() - start_time
        
        # ì •í™•ë„ ê³„ì‚°
        correct = sum(1 for true, pred in zip(sample_labels, predictions) if true == pred)
        accuracy = correct / len(sample_labels)
        
        print(f"âœ… {model_name} ê²°ê³¼:")
        print(f"   ì •í™•ë„: {accuracy:.3f} (ìƒ˜í”Œ {sample_size}ê°œ)")
        print(f"   ì¶”ë¡  ì‹œê°„: {inference_time:.2f}ì´ˆ")
        print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„/í…ìŠ¤íŠ¸: {inference_time/sample_size:.3f}ì´ˆ")
        
        # ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
        print(f"   ì˜ˆì¸¡ ìƒ˜í”Œ:")
        for i in range(min(3, len(sample_texts))):
            print(f"      ì‹¤ì œ: {sample_labels[i]} â†’ ì˜ˆì¸¡: {predictions[i]}")
        
        return accuracy
        
    except Exception as e:
        print(f"âŒ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return 0.0

def comprehensive_model_test():
    """ì¢…í•© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
    
    print("ğŸ¯ ë©´ì ‘ ì„±í–¥ ë¶„ì„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ë°ì´í„° ë¡œë“œ
    try:
        texts, labels = load_interview_data()
        print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(texts)}ê°œ ìƒ˜í”Œ, {len(set(labels))}ê°œ ì„±í–¥")
        
        # ì„±í–¥ë³„ ë¶„í¬ í™•ì¸
        label_counts = pd.Series(labels).value_counts()
        print(f"ì„±í–¥ë³„ ë¶„í¬:")
        for personality, count in label_counts.items():
            print(f"   {personality}: {count}ê°œ")
        
    except FileNotFoundError:
        print("âŒ interview_dataset.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € interview_data_generator.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    print(f"\nğŸ“ˆ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• : {len(X_train)}/{len(X_test)}")
    
    # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    results = {}
    
    # 1. ê¸°ì¤€ ëª¨ë¸
    baseline_acc = test_baseline_model(X_train, X_test, y_train, y_test)
    results["Baseline (TF-IDF + LR)"] = baseline_acc
    
    # 2. Hugging Face ëª¨ë¸ë“¤
    models_to_test = [
        "cardiffnlp/twitter-roberta-base-sentiment-latest",
        "facebook/bart-large-mnli"
    ]
    
    for model_name in models_to_test:
        try:
            acc = test_huggingface_model(texts, labels, model_name)
            results[model_name] = acc
        except Exception as e:
            print(f"âš ï¸ {model_name} ê±´ë„ˆë›°ê¸°: {str(e)}")
    
    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ† ì¢…í•© ê²°ê³¼:")
    print("=" * 50)
    for model, accuracy in sorted(results.items(), key=lambda x: x[1], reverse=True):
        print(f"{model}: {accuracy:.3f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
    best_model = max(results.items(), key=lambda x: x[1])
    print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥: {best_model[0]} (ì •í™•ë„: {best_model[1]:.3f})")
    
    # ê°œì„  ë°©ì•ˆ ì œì•ˆ
    print(f"\nğŸ’¡ ê°œì„  ë°©ì•ˆ:")
    print(f"   1. ë” ë§ì€ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘")
    print(f"   2. í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ í™œìš© (KcELECTRA, KoBERT)")
    print(f"   3. íŒŒì¸íŠœë‹ìœ¼ë¡œ ì„±í–¥ ë¶„ë¥˜ íŠ¹í™”")
    print(f"   4. ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±")

if __name__ == "__main__":
    comprehensive_model_test()